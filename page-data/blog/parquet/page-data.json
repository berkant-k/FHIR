{"componentChunkName":"component---src-pages-blog-parquet-md","path":"/blog/parquet/","result":{"pageContext":{"frontmatter":{"layout":"post","title":"Bulk export - Converting NDJSON to Parquet","description":"Bulk export - Converting NDJSON to Parquet","date":"2022-05-09T00:00:00.000Z"},"relativePagePath":"/blog/parquet.md","titleType":"append","MdxNode":{"id":"2bcdc213-d7c6-52c1-9533-d84b1b1dc7ee","children":[],"parent":"d1b7da74-d4fe-583d-a2b1-053d14d2b1b0","internal":{"content":"---\nlayout: post\ntitle:  Bulk export - Converting NDJSON to Parquet\ndescription: Bulk export - Converting NDJSON to Parquet\ndate:   2022-05-09\n---\n\nBy Lee Surprenant    |    Published May 10, 2022\n\n# Background\nIn IBM FHIR Server 4.4.0, we introduced experimental support for [\"export to parquet\"](https://github.com/LinuxForHealth/FHIR/issues/1340). The feature was implemented by embedding a single-node Apache Spark cluster and using it to:\n1. infer a schema from a collection of JSON resources;\n2. write Parquet to Amazon S3 / IBM Cloud Object Storage.\n\nI planned to either split this into a separate component or use an external Spark service for this feature (or both!), but the demand for the feature has not warranted the investment that would require.\nThus, beginning with IBM FHIR Server 4.11.0, the \"export to parquet\" feature has been removed.\n\nBut fear not, the LinuxForHealth FHIR Server still supports exporting to newline-delimited JSON (NDJSON) on Amazon S3 / IBM Cloud Object Storage and users with access to the bucket can use these same Spark features to convert from NDJSON to Parquet.\n\n# Bulk Export\nBulk export can be performed via HTTP GET or POST and the LinuxForHealth FHIR Server supports three flavors:\n* System export:  `[base]/$export`\n* Patient export:  `[base]/Patient/$export`\n* Group export:  `[base]/Group/[id]/$export`\n\nThe export operations are defined at https://hl7.org/fhir/uv/bulkdata/export.html and usage information can be found in the LinuxForHealth FHIR Server [Bulk Data Guide](https://linuxforhealth.github.io/FHIR/guides/FHIRBulkOperations#export-operation-dollarexport).\n\nFor example, to export all Patient and Condition resources from an IBM FHIR Server at example.com:\n```\ncurl --request POST \\\n  --url 'https://example.com/fhir-server/api/v4/$export' \\\n  --header 'Authorization: *****' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n   \"resourceType\": \"Parameters\",\n   \"parameter\": [\n       {\n           \"name\": \"_type\",\n           \"valueString\": \"Patient,Condition\"\n       }\n   ]\n}'\n```\n\nBy default, the LinuxForHealth FHIR Server uses a psuedo-folder structure for the output files of each job. In the example above, it might produce output files like the following in the configured bucket:\n* long-job-id/Condition_1.ndjson\n* long-job-id/Condition_2.ndjson\n* long-job-id/Condition_3.ndjson\n* long-job-id/Patient_1.ndjson\n\nNormally, a client would retrieve the exported NDJSON data from the download urls obtained from the `$bulkdata-status` URL in the Location header of the $export response. Users could then copy those files to their own S3 / Cloud Object Storage bucket (or any other Hadoop-compatible storage) for analysis.\nAlternatively, privileged users with access to the export bucket can operate directly over the exported files.\n\n# Convert from NDJSON to Parquet via Apache Spark\nGiven a properly configured Spark environment, converting the exported NDJSON files to Parquet can be done in just a few lines of code.\n\nFor example, using pyspark to operate over data in IBM Cloud Object storage in \"us-east\":\n```\nimport ibmos2spark\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\ncredentials = {\n    'service_id': cos_api_key['iam_serviceid_crn'],\n    'api_key': cos_api_key['apikey'],\n    'endpoint': 'https://s3.private.us-east.cloud-object-storage.appdomain.cloud',\n    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token'\n}\nconfiguration_name = 'your_config_name'  # Must be unique for each bucket / configuration!\nspark_cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nbucket = 'unique-export-bucket'  # The globally-unique bucket with the exported NDJSON\nin_files = 'long-job-id/Condition_*.ndjson'  # Note the wildcard pattern!\ncondition = spark.read.format('json').load(spark_cos.url(in_files, bucket))\n\nout_file = 'condition.parquet'\ncondition.write.parquet(spark_cos.url(out_file, bucket))\n```\n\nThe initial read may take some time as Spark must infer the schema from the data.\nHowever, that schema will be saved to the parquet output and, from there, the data can be loaded very quickly.\n\nSpark automatically splits the data into a number of reasonably-sized parquet files (called \"bucketing\"), but it also provides configuration options so that you can optimize the parquet storage for your particular use.\n\n# Working with FHIR data from Apache Spark\nNow that you have the data in a format that works well with Spark, you can use Spark to shape / transform the data into whatever format is most useful for your project. For an example, check out the recording from our FHIR DevDays presentation [FHIR from Jupyter](https://youtu.be/CZe48jUzNY0?t=1149) or jump straight to [the notebooks](https://github.com/Alvearie/FHIR-from-Jupyter).\n","type":"Mdx","contentDigest":"d0b8104deb5d06ac2203fe41c4c62636","owner":"gatsby-plugin-mdx","counter":98,"fieldOwners":{"slug":"gatsby-plugin-slug"}},"frontmatter":{"title":"Bulk export - Converting NDJSON to Parquet","layout":"post","description":"Bulk export - Converting NDJSON to Parquet","date":"2022-05-09T00:00:00.000Z"},"exports":{},"rawBody":"---\nlayout: post\ntitle:  Bulk export - Converting NDJSON to Parquet\ndescription: Bulk export - Converting NDJSON to Parquet\ndate:   2022-05-09\n---\n\nBy Lee Surprenant    |    Published May 10, 2022\n\n# Background\nIn IBM FHIR Server 4.4.0, we introduced experimental support for [\"export to parquet\"](https://github.com/LinuxForHealth/FHIR/issues/1340). The feature was implemented by embedding a single-node Apache Spark cluster and using it to:\n1. infer a schema from a collection of JSON resources;\n2. write Parquet to Amazon S3 / IBM Cloud Object Storage.\n\nI planned to either split this into a separate component or use an external Spark service for this feature (or both!), but the demand for the feature has not warranted the investment that would require.\nThus, beginning with IBM FHIR Server 4.11.0, the \"export to parquet\" feature has been removed.\n\nBut fear not, the LinuxForHealth FHIR Server still supports exporting to newline-delimited JSON (NDJSON) on Amazon S3 / IBM Cloud Object Storage and users with access to the bucket can use these same Spark features to convert from NDJSON to Parquet.\n\n# Bulk Export\nBulk export can be performed via HTTP GET or POST and the LinuxForHealth FHIR Server supports three flavors:\n* System export:  `[base]/$export`\n* Patient export:  `[base]/Patient/$export`\n* Group export:  `[base]/Group/[id]/$export`\n\nThe export operations are defined at https://hl7.org/fhir/uv/bulkdata/export.html and usage information can be found in the LinuxForHealth FHIR Server [Bulk Data Guide](https://linuxforhealth.github.io/FHIR/guides/FHIRBulkOperations#export-operation-dollarexport).\n\nFor example, to export all Patient and Condition resources from an IBM FHIR Server at example.com:\n```\ncurl --request POST \\\n  --url 'https://example.com/fhir-server/api/v4/$export' \\\n  --header 'Authorization: *****' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n   \"resourceType\": \"Parameters\",\n   \"parameter\": [\n       {\n           \"name\": \"_type\",\n           \"valueString\": \"Patient,Condition\"\n       }\n   ]\n}'\n```\n\nBy default, the LinuxForHealth FHIR Server uses a psuedo-folder structure for the output files of each job. In the example above, it might produce output files like the following in the configured bucket:\n* long-job-id/Condition_1.ndjson\n* long-job-id/Condition_2.ndjson\n* long-job-id/Condition_3.ndjson\n* long-job-id/Patient_1.ndjson\n\nNormally, a client would retrieve the exported NDJSON data from the download urls obtained from the `$bulkdata-status` URL in the Location header of the $export response. Users could then copy those files to their own S3 / Cloud Object Storage bucket (or any other Hadoop-compatible storage) for analysis.\nAlternatively, privileged users with access to the export bucket can operate directly over the exported files.\n\n# Convert from NDJSON to Parquet via Apache Spark\nGiven a properly configured Spark environment, converting the exported NDJSON files to Parquet can be done in just a few lines of code.\n\nFor example, using pyspark to operate over data in IBM Cloud Object storage in \"us-east\":\n```\nimport ibmos2spark\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\ncredentials = {\n    'service_id': cos_api_key['iam_serviceid_crn'],\n    'api_key': cos_api_key['apikey'],\n    'endpoint': 'https://s3.private.us-east.cloud-object-storage.appdomain.cloud',\n    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token'\n}\nconfiguration_name = 'your_config_name'  # Must be unique for each bucket / configuration!\nspark_cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nbucket = 'unique-export-bucket'  # The globally-unique bucket with the exported NDJSON\nin_files = 'long-job-id/Condition_*.ndjson'  # Note the wildcard pattern!\ncondition = spark.read.format('json').load(spark_cos.url(in_files, bucket))\n\nout_file = 'condition.parquet'\ncondition.write.parquet(spark_cos.url(out_file, bucket))\n```\n\nThe initial read may take some time as Spark must infer the schema from the data.\nHowever, that schema will be saved to the parquet output and, from there, the data can be loaded very quickly.\n\nSpark automatically splits the data into a number of reasonably-sized parquet files (called \"bucketing\"), but it also provides configuration options so that you can optimize the parquet storage for your particular use.\n\n# Working with FHIR data from Apache Spark\nNow that you have the data in a format that works well with Spark, you can use Spark to shape / transform the data into whatever format is most useful for your project. For an example, check out the recording from our FHIR DevDays presentation [FHIR from Jupyter](https://youtu.be/CZe48jUzNY0?t=1149) or jump straight to [the notebooks](https://github.com/Alvearie/FHIR-from-Jupyter).\n","fileAbsolutePath":"/home/runner/work/FHIR/FHIR/fhir/docs/src/pages/blog/parquet.md","fields":{"slug":"/blog/parquet"}}}},"staticQueryHashes":["1364590287","2102389209","2102389209","227138135","227138135","2456312558","2746626797","2746626797","3018647132","3018647132","3906363820","3906363820","768070550"]}